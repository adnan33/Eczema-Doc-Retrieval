{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json, pickle, ijson,json\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified bioclean: also split on dashes. Works better for retrieval with galago.\n",
    "bioclean_mod = lambda t: re.sub(\n",
    "    '[.,?;*!%^&_+():-\\[\\]{}]', '',\n",
    "    t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').replace(\"-\", ' ').strip().lower()\n",
    ").split()\n",
    "bioclean    = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip().lower()).split()\n",
    "\n",
    "doc_index = 'pubmed_abstracts_index_0_1'\n",
    "map         = \"pubmed_abstracts_mapping_0_1\"\n",
    "def idf_val(w, idf, max_idf):\n",
    "    if w in idf:\n",
    "        return idf[w]\n",
    "    return max_idf\n",
    "\n",
    "def tokenize(x):\n",
    "  return bioclean(x)\n",
    "\n",
    "def GetWords(data, doc_text, words):\n",
    "  for i in range(len(data['queries'])):\n",
    "    qwds = tokenize(data['queries'][i]['query_text'])\n",
    "    for w in qwds:\n",
    "      words[w] = 1\n",
    "    for j in range(len(data['queries'][i]['retrieved_documents'])):\n",
    "      doc_id = data['queries'][i]['retrieved_documents'][j]['doc_id']\n",
    "      dtext = (\n",
    "              doc_text[doc_id]['title'] + ' <title> ' + doc_text[doc_id]['abstractText']\n",
    "              # +\n",
    "              # ' '.join(\n",
    "              #     [\n",
    "              #         ' '.join(mm) for mm in\n",
    "              #         get_the_mesh(doc_text[doc_id])\n",
    "              #     ]\n",
    "              # )\n",
    "      )\n",
    "      dwds = tokenize(dtext)\n",
    "      for w in dwds:\n",
    "        words[w] = 1\n",
    "\n",
    "def load_idfs(idf_path, words):\n",
    "    print('Loading IDF tables')\n",
    "    #\n",
    "    # with open(dataloc + 'idf.pkl', 'rb') as f:\n",
    "    with open(idf_path, 'rb') as f:\n",
    "        idf = pickle.load(f)\n",
    "    ret = {}\n",
    "    for w in words:\n",
    "        if w in idf:\n",
    "            ret[w] = idf[w]\n",
    "    max_idf = 0.0\n",
    "    for w in idf:\n",
    "        if idf[w] > max_idf:\n",
    "            max_idf = idf[w]\n",
    "    idf = None\n",
    "    print('Loaded idf tables with max idf {}'.format(max_idf))\n",
    "    #\n",
    "    return ret, max_idf\n",
    "\n",
    "def load_all_data(dataloc, idf_pickle_path):\n",
    "    print('loading pickle data')\n",
    "    #\n",
    "    with open(dataloc+'trainining7b.json', 'r') as f:\n",
    "        bioasq7_data = json.load(f)\n",
    "        bioasq7_data = dict((q['id'], q) for q in bioasq7_data['questions'])\n",
    "    #\n",
    "    with open(dataloc + 'bioasq7_bm25_top100.dev.pkl', 'rb') as f:\n",
    "        dev_data = pickle.load(f)\n",
    "    with open(dataloc + 'bioasq7_bm25_docset_top100.dev.pkl', 'rb') as f:\n",
    "        dev_docs = pickle.load(f)\n",
    "    with open(dataloc + 'bioasq7_bm25_top100.train.pkl', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(dataloc + 'bioasq7_bm25_docset_top100.train.pkl', 'rb') as f:\n",
    "        train_docs = pickle.load(f)\n",
    "    print('loading words')\n",
    "    #\n",
    "    words               = {}\n",
    "    GetWords(train_data, train_docs, words)\n",
    "    GetWords(dev_data,   dev_docs,   words)\n",
    "    #\n",
    "    print('loading idfs')\n",
    "    idf, max_idf    = load_idfs(idf_pickle_path, words)\n",
    "    return dev_data, dev_docs, train_data, train_docs, idf, max_idf, bioasq7_data\n",
    "\n",
    "# recall: 0.3883\n",
    "def get_first_n_20(qtext, n, max_year=2019):\n",
    "    #\n",
    "    tokenized_body  = bioclean_mod(qtext)\n",
    "    question_tokens = [t for t in tokenized_body if t not in stopwords]\n",
    "    idf_scores      = [idf_val(w, idf, max_idf) for w in question_tokens]\n",
    "    question        = ' '.join(question_tokens)\n",
    "    #\n",
    "    the_shoulds = []\n",
    "    for q_tok, idf_score in zip(question_tokens, idf_scores):\n",
    "        the_shoulds.append({\"match\": {\"joint_text\"                  : {\"query\": q_tok, \"boost\": idf_score}}})\n",
    "        the_shoulds.append({\"match\": {\"Chemicals.NameOfSubstance\"   : {\"query\": q_tok, \"boost\": idf_score}}})\n",
    "        the_shoulds.append({\"match\": {\"MeshHeadings.text\"           : {\"query\": q_tok, \"boost\": idf_score}}})\n",
    "        the_shoulds.append({\"match\": {\"SupplMeshList.text\"          : {\"query\": q_tok, \"boost\": idf_score}}})\n",
    "        ################################################\n",
    "        the_shoulds.append({\"terms\": {\"joint_text\"                  : [q_tok], \"boost\": idf_score}})\n",
    "        the_shoulds.append({\"terms\": {\"Chemicals.NameOfSubstance\"   : [q_tok], \"boost\": idf_score}})\n",
    "        the_shoulds.append({\"terms\": {\"MeshHeadings.text\"           : [q_tok], \"boost\": idf_score}})\n",
    "        the_shoulds.append({\"terms\": {\"joint_text\"                  : [q_tok], \"boost\": idf_score}})\n",
    "    ################################################\n",
    "    if(len(question_tokens) > 1):\n",
    "        the_shoulds.append({\"span_near\": {\"clauses\": [{\"span_term\": {\"joint_text\": w}} for w in question_tokens], \"slop\": 5, \"in_order\": False}})\n",
    "    ################################################\n",
    "    bod         = {\n",
    "        \"size\": n,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [{\"range\":{\"DateCompleted\": {\"gte\": \"1800\", \"lte\": str(max_year), \"format\": \"dd/MM/yyyy||yyyy\"}}}],\n",
    "                \"should\": [\n",
    "                    {\"match\":{\"joint_text\": {\"query\": question, \"boost\": sum(idf_scores)}}},\n",
    "                ]+the_shoulds,\n",
    "                \"minimum_should_match\": 1,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(bod))\n",
    "    res         = es.search(index=doc_index, body=bod, request_timeout=120)\n",
    "    print(res)\n",
    "    return res['hits']['hits']\n",
    "\n",
    "# recall: 0.4140\n",
    "def get_first_n_1(qtext, n, max_year=2017):\n",
    "    tokenized_body  = bioclean_mod(qtext)\n",
    "    tokenized_body  = [t for t in tokenized_body if t not in stopwords]\n",
    "    question        = ' '.join(tokenized_body)\n",
    "    ################################################\n",
    "    bod         = {\n",
    "        \"size\": n,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [{\"range\": {\"DateCompleted\": {\"gte\": \"1800\", \"lte\": str(max_year), \"format\": \"dd/MM/yyyy||yyyy\"}}}],\n",
    "                \"should\": [{\"match\": {\"joint_text\": {\"query\": question, \"boost\": 1}}}],\n",
    "                \"minimum_should_match\": 1,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(bod))\n",
    "    res         = es.search(index=doc_index,  body=bod, request_timeout=120)\n",
    "    print(res)\n",
    "    return res['hits']['hits']\n",
    "\n",
    "# recall: 0.4144\n",
    "def get_first_n_2(qtext, n, max_year=2017):\n",
    "    tokenized_body      = bioclean_mod(qtext)\n",
    "    question_tokens     = [t for t in tokenized_body if t not in stopwords]\n",
    "    question            = ' '.join(question_tokens)\n",
    "    ################################################\n",
    "    the_shoulds     = []\n",
    "    if(len(question_tokens) > 1):\n",
    "        the_shoulds.append({\"span_near\": {\"clauses\": [{\"span_term\": {\"joint_text\": w}} for w in question_tokens], \"slop\": 5, \"in_order\": False}})\n",
    "    ################################################\n",
    "    bod         = {\n",
    "        \"size\": n,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [{\"range\": {\"DateCompleted\": {\"gte\": \"1800\", \"lte\": str(max_year), \"format\": \"dd/MM/yyyy||yyyy\"}}}],\n",
    "                \"should\": [{\"match\": {\"joint_text\": {\"query\": question, \"boost\": 1}}}] + the_shoulds,\n",
    "                \"minimum_should_match\": 1,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(bod))\n",
    "    res         = es.search(index=doc_index, body=bod, request_timeout=120)\n",
    "    print(res)\n",
    "    return res['hits']['hits']\n",
    "\n",
    "# recall: 0.4150\n",
    "def get_first_n_3(qtext, n, max_year=2020):\n",
    "    tokenized_body  = bioclean_mod(qtext)\n",
    "    question_tokens = [t for t in tokenized_body if t not in stopwords]\n",
    "    question        = ' '.join(question_tokens)\n",
    "    ################################################\n",
    "    the_shoulds = []\n",
    "    the_shoulds.append({\"match\": {\"Chemicals.NameOfSubstance\"   : {\"query\": question}}})\n",
    "    the_shoulds.append({\"match\": {\"MeshHeadings.text\"           : {\"query\": question}}})\n",
    "    the_shoulds.append({\"match\": {\"SupplMeshList.text\"          : {\"query\": question}}})\n",
    "    ################################################\n",
    "    the_shoulds     = []\n",
    "    if(len(question_tokens) > 1):\n",
    "        the_shoulds.append({\"span_near\": {\"clauses\": [{\"span_term\": {\"joint_text\": w}} for w in question_tokens], \"slop\": 5, \"in_order\": False}})\n",
    "    ################################################\n",
    "    bod         = {\n",
    "        \"size\": n,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [{\"range\": {\"DateCompleted\": {\"gte\": \"1800\", \"lte\": str(max_year), \"format\": \"dd/MM/yyyy||yyyy\"}}}],\n",
    "                \"should\": [{\"match\": {\"joint_text\": {\"query\": question, \"boost\": 1}}}] + the_shoulds,\n",
    "                \"minimum_should_match\": 1,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(bod))\n",
    "    res         = es.search(index=doc_index, body=bod, request_timeout=120)\n",
    "    print(res)\n",
    "    return res['hits']['hits']\n",
    "\n",
    "def get_multi(qtext,n, max_year=2020):\n",
    "    tokenized_body  = bioclean_mod(qtext)\n",
    "    question_tokens = [t for t in tokenized_body if t not in stopwords]\n",
    "    question        = ' '.join(question_tokens)\n",
    "    bod = {\n",
    "    \"size\": n,\n",
    "   \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": question,\n",
    "            \"type\":       \"most_fields\",\n",
    "            \"fields\": [\"AbstractText\",\"ArticleTitle\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "        \n",
    "    res         = es.search(index=doc_index, body=bod, request_timeout=120)\n",
    "    return res['hits']['hits']\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickle data\n",
      "loading words\n",
      "loading idfs\n",
      "Loading IDF tables\n",
      "Loaded idf tables with max idf 16.5163157103\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\n",
    "   ['localhost:9200'],\n",
    "    verify_certs        = True,\n",
    "    timeout             = 150,\n",
    "    max_retries         = 10,\n",
    "    retry_on_timeout    = True\n",
    ")\n",
    "\n",
    "dataloc='./Data/bioasq_data/'\n",
    "w2v_bin_path                = './Data/PretrainedWeightsAndVectors/pubmed2018_w2v_30D.bin'\n",
    "idf_pickle_path             = './Data/PretrainedWeightsAndVectors/idf.pkl'\n",
    "(dev_data, dev_docs, train_data, train_docs, idf, max_idf, bioasq7_data) = load_all_data(dataloc, idf_pickle_path)\n",
    "\n",
    "with open('./Data/stopwords.pkl', 'rb') as f:\n",
    "    stopwords = pickle.load(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "\n",
      "  2%|█▋                                                                                | 2/100 [00:00<00:09, 10.20it/s]\n",
      "\n",
      "  3%|██▍                                                                               | 3/100 [00:00<00:10,  9.22it/s]\n",
      "\n",
      "  6%|████▉                                                                             | 6/100 [00:02<00:27,  3.44it/s]\n",
      "\n",
      "  7%|█████▋                                                                            | 7/100 [00:03<00:38,  2.44it/s]\n",
      "\n",
      "  8%|██████▌                                                                           | 8/100 [00:03<00:29,  3.16it/s]\n",
      "\n",
      "  9%|███████▍                                                                          | 9/100 [00:03<00:24,  3.68it/s]\n",
      "\n",
      " 10%|████████                                                                         | 10/100 [00:03<00:23,  3.87it/s]\n",
      "\n",
      " 11%|████████▉                                                                        | 11/100 [00:03<00:19,  4.68it/s]\n",
      "\n",
      " 13%|██████████▌                                                                      | 13/100 [00:03<00:14,  5.99it/s]\n",
      "\n",
      " 14%|███████████▎                                                                     | 14/100 [00:04<00:13,  6.45it/s]\n",
      "\n",
      " 16%|████████████▉                                                                    | 16/100 [00:08<01:01,  1.37it/s]\n",
      "\n",
      " 17%|█████████████▊                                                                   | 17/100 [00:13<02:55,  2.12s/it]\n",
      "\n",
      " 18%|██████████████▌                                                                  | 18/100 [00:16<03:15,  2.39s/it]\n",
      "\n",
      " 19%|███████████████▍                                                                 | 19/100 [00:21<04:04,  3.01s/it]\n",
      "\n",
      " 20%|████████████████▏                                                                | 20/100 [00:24<04:10,  3.13s/it]\n",
      "\n",
      " 21%|█████████████████                                                                | 21/100 [00:28<04:31,  3.44s/it]\n",
      "\n",
      " 22%|█████████████████▊                                                               | 22/100 [00:32<04:50,  3.72s/it]\n",
      "\n",
      " 23%|██████████████████▋                                                              | 23/100 [00:35<04:10,  3.26s/it]\n",
      "\n",
      " 24%|███████████████████▍                                                             | 24/100 [00:42<05:41,  4.50s/it]\n",
      "\n",
      " 25%|████████████████████▎                                                            | 25/100 [00:45<05:13,  4.17s/it]\n",
      "\n",
      " 26%|█████████████████████                                                            | 26/100 [00:50<05:14,  4.25s/it]\n",
      "\n",
      " 27%|█████████████████████▊                                                           | 27/100 [00:53<04:39,  3.83s/it]\n",
      "\n",
      " 28%|██████████████████████▋                                                          | 28/100 [00:57<04:47,  3.99s/it]\n",
      "\n",
      " 29%|███████████████████████▍                                                         | 29/100 [01:02<05:10,  4.38s/it]\n",
      "\n",
      " 30%|████████████████████████▎                                                        | 30/100 [01:06<04:43,  4.04s/it]\n",
      "\n",
      " 31%|█████████████████████████                                                        | 31/100 [01:09<04:23,  3.82s/it]\n",
      "\n",
      " 32%|█████████████████████████▉                                                       | 32/100 [01:12<03:57,  3.49s/it]\n",
      "\n",
      " 33%|██████████████████████████▋                                                      | 33/100 [01:14<03:39,  3.28s/it]\n",
      "\n",
      " 34%|███████████████████████████▌                                                     | 34/100 [01:17<03:13,  2.94s/it]\n",
      "\n",
      " 35%|████████████████████████████▎                                                    | 35/100 [01:20<03:14,  2.99s/it]\n",
      "\n",
      " 36%|█████████████████████████████▏                                                   | 36/100 [01:22<03:07,  2.93s/it]\n",
      "\n",
      " 37%|█████████████████████████████▉                                                   | 37/100 [01:26<03:10,  3.02s/it]\n",
      "\n",
      " 38%|██████████████████████████████▊                                                  | 38/100 [01:28<02:57,  2.87s/it]\n",
      "\n",
      " 39%|███████████████████████████████▌                                                 | 39/100 [01:33<03:24,  3.35s/it]\n",
      "\n",
      " 40%|████████████████████████████████▍                                                | 40/100 [01:38<04:01,  4.03s/it]\n",
      "\n",
      " 41%|█████████████████████████████████▏                                               | 41/100 [01:42<03:52,  3.95s/it]\n",
      "\n",
      " 42%|██████████████████████████████████                                               | 42/100 [01:44<03:14,  3.36s/it]\n",
      "\n",
      " 43%|██████████████████████████████████▊                                              | 43/100 [01:47<03:08,  3.31s/it]\n",
      "\n",
      " 44%|███████████████████████████████████▋                                             | 44/100 [01:50<03:03,  3.28s/it]\n",
      "\n",
      " 45%|████████████████████████████████████▍                                            | 45/100 [01:54<03:02,  3.31s/it]\n",
      "\n",
      " 46%|█████████████████████████████████████▎                                           | 46/100 [01:57<02:50,  3.15s/it]\n",
      "\n",
      " 47%|██████████████████████████████████████                                           | 47/100 [02:01<03:12,  3.64s/it]\n",
      "\n",
      " 48%|██████████████████████████████████████▉                                          | 48/100 [02:05<03:02,  3.51s/it]\n",
      "\n",
      " 49%|███████████████████████████████████████▋                                         | 49/100 [02:08<02:58,  3.51s/it]\n",
      "\n",
      " 50%|████████████████████████████████████████▌                                        | 50/100 [02:11<02:46,  3.33s/it]\n",
      "\n",
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [02:14<02:43,  3.33s/it]\n",
      "\n",
      " 52%|██████████████████████████████████████████                                       | 52/100 [02:18<02:43,  3.42s/it]\n",
      "\n",
      " 53%|██████████████████████████████████████████▉                                      | 53/100 [02:22<02:49,  3.60s/it]\n",
      "\n",
      " 54%|███████████████████████████████████████████▋                                     | 54/100 [02:26<02:49,  3.68s/it]\n",
      "\n",
      " 55%|████████████████████████████████████████████▌                                    | 55/100 [02:30<02:49,  3.77s/it]\n",
      "\n",
      " 56%|█████████████████████████████████████████████▎                                   | 56/100 [02:32<02:29,  3.40s/it]\n",
      "\n",
      " 57%|██████████████████████████████████████████████▏                                  | 57/100 [02:37<02:40,  3.74s/it]\n",
      "\n",
      " 58%|██████████████████████████████████████████████▉                                  | 58/100 [02:39<02:16,  3.26s/it]\n",
      "\n",
      " 59%|███████████████████████████████████████████████▊                                 | 59/100 [02:42<02:07,  3.11s/it]\n",
      "\n",
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [02:45<02:05,  3.15s/it]\n",
      "\n",
      " 61%|█████████████████████████████████████████████████▍                               | 61/100 [02:50<02:21,  3.62s/it]\n",
      "\n",
      " 62%|██████████████████████████████████████████████████▏                              | 62/100 [02:53<02:17,  3.62s/it]\n",
      "\n",
      " 63%|███████████████████████████████████████████████████                              | 63/100 [02:57<02:09,  3.50s/it]\n",
      "\n",
      " 64%|███████████████████████████████████████████████████▊                             | 64/100 [03:00<01:59,  3.32s/it]\n",
      "\n",
      " 65%|████████████████████████████████████████████████████▋                            | 65/100 [03:03<01:57,  3.37s/it]\n",
      "\n",
      " 66%|█████████████████████████████████████████████████████▍                           | 66/100 [03:07<02:00,  3.54s/it]\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████▎                          | 67/100 [03:10<01:47,  3.27s/it]\n",
      "\n",
      " 68%|███████████████████████████████████████████████████████                          | 68/100 [03:15<02:02,  3.82s/it]\n",
      "\n",
      " 69%|███████████████████████████████████████████████████████▉                         | 69/100 [03:18<01:52,  3.63s/it]\n",
      "\n",
      " 70%|████████████████████████████████████████████████████████▋                        | 70/100 [03:21<01:41,  3.39s/it]\n",
      "\n",
      " 71%|█████████████████████████████████████████████████████████▌                       | 71/100 [03:24<01:40,  3.45s/it]\n",
      "\n",
      " 72%|██████████████████████████████████████████████████████████▎                      | 72/100 [03:27<01:28,  3.18s/it]\n",
      "\n",
      " 73%|███████████████████████████████████████████████████████████▏                     | 73/100 [03:29<01:18,  2.89s/it]\n",
      "\n",
      " 74%|███████████████████████████████████████████████████████████▉                     | 74/100 [03:34<01:30,  3.48s/it]\n",
      "\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 75/100 [03:38<01:28,  3.55s/it]\n",
      "\n",
      " 76%|█████████████████████████████████████████████████████████████▌                   | 76/100 [03:41<01:25,  3.54s/it]\n",
      "\n",
      " 77%|██████████████████████████████████████████████████████████████▎                  | 77/100 [03:44<01:18,  3.41s/it]\n",
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▏                 | 78/100 [03:47<01:07,  3.07s/it]\n",
      "\n",
      " 79%|███████████████████████████████████████████████████████████████▉                 | 79/100 [03:50<01:10,  3.33s/it]\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████████████▊                | 80/100 [03:54<01:07,  3.36s/it]\n",
      "\n",
      " 81%|█████████████████████████████████████████████████████████████████▌               | 81/100 [03:56<00:59,  3.12s/it]\n",
      "\n",
      " 82%|██████████████████████████████████████████████████████████████████▍              | 82/100 [04:00<00:56,  3.13s/it]\n",
      "\n",
      " 83%|███████████████████████████████████████████████████████████████████▏             | 83/100 [04:02<00:49,  2.90s/it]\n",
      "\n",
      " 84%|████████████████████████████████████████████████████████████████████             | 84/100 [04:06<00:50,  3.13s/it]\n",
      "\n",
      " 85%|████████████████████████████████████████████████████████████████████▊            | 85/100 [04:07<00:41,  2.74s/it]\n",
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████▋           | 86/100 [04:11<00:40,  2.86s/it]\n",
      "\n",
      " 87%|██████████████████████████████████████████████████████████████████████▍          | 87/100 [04:18<00:53,  4.13s/it]\n",
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▎         | 88/100 [04:21<00:46,  3.90s/it]\n",
      "\n",
      " 89%|████████████████████████████████████████████████████████████████████████         | 89/100 [04:26<00:44,  4.06s/it]\n",
      "\n",
      " 90%|████████████████████████████████████████████████████████████████████████▉        | 90/100 [04:29<00:40,  4.03s/it]\n",
      "\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▋       | 91/100 [04:34<00:38,  4.31s/it]\n",
      "\n",
      " 92%|██████████████████████████████████████████████████████████████████████████▌      | 92/100 [04:37<00:30,  3.86s/it]\n",
      "\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▎     | 93/100 [04:40<00:24,  3.48s/it]\n",
      "\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▏    | 94/100 [04:42<00:18,  3.13s/it]\n",
      "\n",
      " 95%|████████████████████████████████████████████████████████████████████████████▉    | 95/100 [04:46<00:16,  3.32s/it]\n",
      "\n",
      " 96%|█████████████████████████████████████████████████████████████████████████████▊   | 96/100 [04:50<00:13,  3.41s/it]\n",
      "\n",
      " 97%|██████████████████████████████████████████████████████████████████████████████▌  | 97/100 [04:50<00:07,  2.60s/it]\n",
      "\n",
      " 98%|███████████████████████████████████████████████████████████████████████████████▍ | 98/100 [04:54<00:05,  2.96s/it]\n",
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████▏| 99/100 [04:54<00:02,  2.13s/it]\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [04:57<00:00,  2.23s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV RECALL\n",
      "0.5099327929645772\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "for q in tqdm(dev_data['queries']):\n",
    "    qtext           = q['query_text']\n",
    "    #####\n",
    "    #results         = get_first_n_20(qtext, 100,2019)\n",
    "    #\n",
    "    #results         = get_first_n_1(qtext, 100,2019)\n",
    "    #results         = get_first_n_2(qtext, 100,2019)\n",
    "    #results         = get_first_n_3(qtext, 100,2020)\n",
    "    results         = get_multi(qtext, 100,2020)\n",
    "    #####\n",
    "    #print(results)\n",
    "    retr_pmids      = [t['_source']['pmid'] for t in results]\n",
    "    #####\n",
    "    rel_ret         = sum([1 if (t in q['relevant_documents']) else 0 for t in retr_pmids])\n",
    "    #####\n",
    "    recall          = float(rel_ret) / float(len(q['relevant_documents']))\n",
    "    recalls.append(recall)\n",
    "    # if(len(recalls) == 100):\n",
    "    #     break\n",
    "\n",
    "print('DEV RECALL')\n",
    "print(sum(recalls) / float(len(recalls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit",
   "language": "python",
   "name": "python361064bitff3277f13ccb44fe8a2fcb98af67160e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
